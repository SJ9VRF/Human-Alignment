# Compiled pipeline definition.

pipelineInfo:
  name: autosxs_pipeline
  description: >
    This pipeline evaluates two models using AutoSxS and compares their alignment
    with human-preference data for question answering.
  inputs:
    - name: evaluation_dataset
      description: Cloud Storage path to the evaluation dataset (JSONL format).
      type: String
    - name: id_columns
      description: List of columns to identify unique examples.
      type: List
    - name: autorater_prompt_parameters
      description: Mapping of prompt parameters to dataset columns/templates.
      type: Dict
    - name: task
      description: Task type (e.g., summarization, question_answering).
      type: String
    - name: response_column_a
      description: Column containing predictions for Model A.
      type: String
    - name: response_column_b
      description: Column containing predictions for Model B.
      type: String
    - name: human_preference_column
      description: Column containing human-preference data.
      type: String

components:
  - name: preprocess_dataset
    description: Preprocess the input dataset for AutoSxS.
    inputs:
      - name: evaluation_dataset
        type: String
      - name: id_columns
        type: List
      - name: autorater_prompt_parameters
        type: Dict
    outputs:
      - name: preprocessed_dataset
        type: String
    implementation:
      container:
        image: gcr.io/google-containers/autosxs-preprocess:latest
        args:
          - --input_dataset
          - {inputValue: evaluation_dataset}
          - --id_columns
          - {inputValue: id_columns}
          - --prompt_parameters
          - {inputValue: autorater_prompt_parameters}
          - --output_path
          - {outputPath: preprocessed_dataset}

  - name: autosxs_evaluation
    description: Run the AutoSxS evaluation for human alignment.
    inputs:
      - name: preprocessed_dataset
        type: String
      - name: task
        type: String
      - name: response_column_a
        type: String
      - name: response_column_b
        type: String
      - name: human_preference_column
        type: String
    outputs:
      - name: judgments
        type: String
      - name: metrics
        type: String
    implementation:
      container:
        image: gcr.io/google-containers/autosxs-evaluation:latest
        args:
          - --dataset
          - {inputValue: preprocessed_dataset}
          - --task
          - {inputValue: task}
          - --response_a
          - {inputValue: response_column_a}
          - --response_b
          - {inputValue: response_column_b}
          - --human_preference
          - {inputValue: human_preference_column}
          - --judgments_output
          - {outputPath: judgments}
          - --metrics_output
          - {outputPath: metrics}

tasks:
  preprocess:
    componentRef:
      name: preprocess_dataset
    inputs:
      evaluation_dataset: {pipelineInput: evaluation_dataset}
      id_columns: {pipelineInput: id_columns}
      autorater_prompt_parameters: {pipelineInput: autorater_prompt_parameters}
    outputs:
      preprocessed_dataset: preprocess_output

  evaluate:
    componentRef:
      name: autosxs_evaluation
    inputs:
      preprocessed_dataset: {taskOutput: preprocess.preprocessed_dataset}
      task: {pipelineInput: task}
      response_column_a: {pipelineInput: response_column_a}
      response_column_b: {pipelineInput: response_column_b}
      human_preference_column: {pipelineInput: human_preference_column}
    outputs:
      judgments: evaluation_judgments
      metrics: evaluation_metrics

outputs:
  - name: judgments
    description: Judgments generated by AutoSxS.
    value: {taskOutput: evaluate.judgments}
  - name: metrics
    description: Metrics comparing AutoRater with human preferences.
    value: {taskOutput: evaluate.metrics}
